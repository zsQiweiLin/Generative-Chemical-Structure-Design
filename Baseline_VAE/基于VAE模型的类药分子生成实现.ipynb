{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "代码用途：用VAE模型从分子数据生成符合类药特性的分子（含5维度评估+结构图展示）\n",
    "核心流程：加载数据→预处理→训练模型→生成分子→评估质量→展示结构\n",
    "    适用环境：PyCharm Jupyter Notebook（必须）+ GPU/CPU\n",
    "    关键注意事项：\n",
    "    1. 运行后会自动生成3个文件：临时生成结果、最终有效分子、评估报告\n",
    "    2. 第一次运行会训练模型，后续运行会加载已保存的模型"
   ],
   "id": "973194af3829eb31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#   1、初始化环境+部分函数构建",
   "id": "2dc1611e018b39dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:16:47.752337Z",
     "start_time": "2025-12-15T08:16:47.743268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 一、核心库导入与基础函数定义（修复 PyTorch 优化器导入）\n",
    "import selfies as sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import Value\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# 修复：替换 from torch.optim import optim → import torch.optim as optim\n",
    "import torch.optim as optim\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski, Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdForceFieldHelpers import UFFOptimizeMolecule\n",
    "import rdkit\n",
    "import os\n",
    "import pickle\n",
    "from IPython.display import display  # Jupyter展示图片必需\n",
    "\n",
    "# 二、核心工具函数定义（数据处理+性质计算）\n",
    "### 1. SA分数计算函数\n",
    "def calculate_SA_score(mol):\n",
    "    if mol is None:\n",
    "        return 10.0\n",
    "    ring_info = mol.GetRingInfo()\n",
    "    num_rings = ring_info.NumRings()\n",
    "    num_heavy_atoms = mol.GetNumHeavyAtoms()\n",
    "    num_bonds = mol.GetNumBonds()\n",
    "    sa_score = 1.0 + 0.15 * num_rings + 0.05 * num_heavy_atoms - 0.02 * num_bonds\n",
    "    return min(max(sa_score, 1.0), 10.0)\n",
    "\n",
    "### 2. 超价原子约束函数\n",
    "def set_universal_bond_constraints():\n",
    "    custom_constraints = sf.get_semantic_constraints()\n",
    "    supervalent_constraints = {\n",
    "        \"S\": 4, \"P\": 5, \"Cl\": 3, \"Br\": 5, \"I\": 6,\n",
    "        \"Se\": 4, \"Te\": 6, \"As\": 5, \"Si\": 6, \"B\": 4\n",
    "    }\n",
    "    custom_constraints.update(supervalent_constraints)\n",
    "    sf.set_semantic_constraints(custom_constraints)\n",
    "set_universal_bond_constraints()\n",
    "\n",
    "### 3. 数据过滤函数（有效性基础检查）\n",
    "def filter_unreasonable_smiles(smiles):\n",
    "    if not isinstance(smiles, str) or len(smiles) < 3 or len(smiles) > 100:\n",
    "        return False\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        return False\n",
    "    atom_count = len(mol.GetAtoms())\n",
    "    if atom_count < 3 or atom_count > 50:\n",
    "        return False\n",
    "    total_charge = sum(atom.GetFormalCharge() for atom in mol.GetAtoms())\n",
    "    if abs(total_charge) > 2:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "### 4. 化学性质计算缓存函数\n",
    "counter = Value('i', 0)\n",
    "@lru_cache(maxsize=30000)\n",
    "def calculate_chemical_properties_cached(smiles):\n",
    "    global counter\n",
    "    with counter.get_lock():\n",
    "        counter.value += 1\n",
    "        if counter.value % 100000 == 0:\n",
    "            print(f\"已预处理 {counter.value} 个分子...\")\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if not mol:\n",
    "        return None\n",
    "    mw = Descriptors.MolWt(mol)\n",
    "    logp = Descriptors.MolLogP(mol)\n",
    "    h_donors = Lipinski.NumHDonors(mol)\n",
    "    h_acceptors = Lipinski.NumHAcceptors(mol)\n",
    "    sa_score = calculate_SA_score(mol)\n",
    "    energy = 300\n",
    "    try:\n",
    "        mol_h = Chem.AddHs(mol)\n",
    "        if AllChem.EmbedMolecule(mol_h, maxAttempts=100) == 0:\n",
    "            mmff_props = AllChem.MMFFGetMoleculeProperties(mol_h)\n",
    "            if mmff_props:\n",
    "                ff = AllChem.MMFFGetMoleculeForceField(mol_h, mmff_props)\n",
    "                ff.Minimize(maxIts=200)\n",
    "                energy = ff.CalcEnergy()\n",
    "    except:\n",
    "        energy = 500\n",
    "    try:\n",
    "        selfies = sf.encoder(smiles, strict=False)\n",
    "        if not selfies:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    return (smiles, selfies, [mw, logp, h_donors, h_acceptors, sa_score, energy])\n",
    "\n",
    "### 5. 批量数据处理函数\n",
    "def batch_process_smiles(smiles_list, n_workers=12):\n",
    "    print(f\"使用 {n_workers} 个线程处理 {len(smiles_list)} 条数据...\")\n",
    "    with ThreadPool(processes=n_workers) as pool:\n",
    "        processed_results = list(tqdm(\n",
    "            pool.imap_unordered(calculate_chemical_properties_cached, smiles_list),\n",
    "            total=len(smiles_list),\n",
    "            desc=\"预处理SMILES→SELFIES+性质计算\",\n",
    "            mininterval=0.5\n",
    "        ))\n",
    "    processed_data = []\n",
    "    for res in processed_results:\n",
    "        if res is not None:\n",
    "            processed_data.append({\"smiles\": res[0], \"selfies\": res[1], \"properties\": res[2]})\n",
    "    print(f\"预处理完成：{len(processed_data)}/{len(smiles_list)} 条数据有效\")\n",
    "    return processed_data\n"
   ],
   "id": "11191736e39a7d45",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#   2、超参数设定与模型构建",
   "id": "47375d2e6d0347df"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:16:47.773662Z",
     "start_time": "2025-12-15T08:16:47.770480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 三、超参数配置（固定不变）\n",
    "os.environ[\"RDKit_SILENT\"] = \"1\"  # 屏蔽RDKit的冗余日志\n",
    "rdkit.RDLogger.DisableLog('rdApp.*')  # 屏蔽RDKit的警告\n",
    "TRAIN_SIZE = 100000  # 训练集数据量\n",
    "VAL_SIZE = 50000  # 验证集数据量\n",
    "BATCH_SIZE = 32  # 每次喂给模型的样本数（固定32，防内存溢出）\n",
    "MAX_SELFIES_LEN = 100  # 分子序列最大长度（超过会截断）\n",
    "FINGERPRINT_NBITS = 256  # 分子指纹维度（评估多样性用，不用改）\n",
    "FINGERPRINT_RADIUS = 1  # 分子指纹半径（固定1）\n",
    "N_WORKERS = 12  # 处理数据的线程数（12线程提速）\n",
    "CACHE_SIZE = 30000  # 性质计算缓存容量（3万条）\n",
    "PREPROCESS_SAVE_PATH = \"preprocess_results.pkl\"  # 预处理结果保存路径（下次不用重复算）\n",
    "model_path = \"best_mol_vae.pth\"  # 最优模型保存路径\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 自动选GPU（快）/CPU（慢）\n"
   ],
   "id": "e12ac0cadd1c8121",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3、数据预处理与加载",
   "id": "8800ef51e0d7db20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:18:02.265405Z",
     "start_time": "2025-12-15T08:16:47.774644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 1. 加载/保存预处理结果\n",
    "print(\"=\"*50)\n",
    "print(\"加载预处理结果（避免重复计算）...\")\n",
    "print(\"=\"*50)\n",
    "if os.path.exists(PREPROCESS_SAVE_PATH):\n",
    "    with open(PREPROCESS_SAVE_PATH, 'rb') as f:\n",
    "        (train_processed, val_processed, train_smiles_raw, val_smiles_raw) = pickle.load(f)\n",
    "    print(f\" 成功加载本地预处理结果！\")\n",
    "    print(f\"训练集有效数据：{len(train_processed)} 条\")\n",
    "    print(f\"验证集有效数据：{len(val_processed)} 条\")\n",
    "else:\n",
    "    print(\"本地无预处理缓存，执行第一次预处理...\")\n",
    "    df = pd.read_csv(\"dataset3/dataset3.csv\")\n",
    "    df.rename(columns={\"0\": \"smiles\", \"1\": \"label\"}, inplace=True)\n",
    "    original_smiles = df[\"smiles\"].tolist()\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(original_smiles)\n",
    "    train_smiles_raw = original_smiles[:TRAIN_SIZE]\n",
    "    val_smiles_raw = original_smiles[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE]\n",
    "\n",
    "    print(\"\\n预处理训练集...\")\n",
    "    counter.value = 0\n",
    "    train_processed = batch_process_smiles(train_smiles_raw)\n",
    "    print(\"\\n预处理验证集...\")\n",
    "    counter.value = 0\n",
    "    val_processed = batch_process_smiles(val_smiles_raw)\n",
    "\n",
    "    with open(PREPROCESS_SAVE_PATH, 'wb') as f:\n",
    "        pickle.dump((train_processed, val_processed, train_smiles_raw, val_smiles_raw), f)\n",
    "    print(f\"\\n 预处理完成并保存到本地！下次运行无需重复计算\")\n",
    "\n",
    "# 提取核心数据\n",
    "train_selfies = [d[\"selfies\"] for d in train_processed]\n",
    "train_properties = [d[\"properties\"] for d in train_processed]\n",
    "train_smiles_valid = [d[\"smiles\"] for d in train_processed]\n",
    "val_selfies = [d[\"selfies\"] for d in val_processed]\n",
    "val_properties = [d[\"properties\"] for d in val_processed]\n",
    "val_smiles_valid = [d[\"smiles\"] for d in val_processed]\n",
    "print(f\"\\n有效训练数据：{len(train_selfies)} 条\")\n",
    "print(f\"有效验证数据：{len(val_selfies)} 条\")\n",
    "\n",
    "### 2. 词汇表构建\n",
    "def build_selfies_vocab(selfies_list):\n",
    "    vocab = defaultdict(int)\n",
    "    special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "    for token in special_tokens:\n",
    "        vocab[token] = len(vocab)\n",
    "    for s in tqdm(selfies_list, desc=\"构建SELFIES词汇表\"):\n",
    "        for char in sf.split_selfies(s):\n",
    "            if char not in vocab:\n",
    "                vocab[char] = len(vocab)\n",
    "    print(f\"词汇表大小：{len(vocab)}\")\n",
    "    return vocab\n",
    "\n",
    "vocab = build_selfies_vocab(train_selfies)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "### 3. SELFIES转序列工具（外部独立函数）\n",
    "def selfies_to_sequence(selfies, vocab, max_len=MAX_SELFIES_LEN):\n",
    "    sequence = [vocab[\"<SOS>\"]]\n",
    "    for char in sf.split_selfies(selfies):\n",
    "        if char in vocab:\n",
    "            sequence.append(vocab[char])\n",
    "    sequence.append(vocab[\"<EOS>\"])\n",
    "    if len(sequence) < max_len:\n",
    "        sequence += [vocab[\"<PAD>\"]] * (max_len - len(sequence))\n",
    "    else:\n",
    "        sequence = sequence[:max_len]\n",
    "    return torch.tensor(sequence, dtype=torch.long)\n",
    "\n",
    "### 4. 数据集类定义（修复 __getitem__ 方法）\n",
    "class SELFIESDataset(Dataset):\n",
    "    def __init__(self, selfies_list, properties_list, vocab):\n",
    "        self.selfies_list = selfies_list\n",
    "        self.properties_list = properties_list\n",
    "        self.vocab = vocab\n",
    "        self.props_mean = torch.tensor(np.mean(properties_list, axis=0), dtype=torch.float32)\n",
    "        self.props_std = torch.tensor(np.std(properties_list, axis=0) + 1e-6, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.selfies_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        selfies = self.selfies_list[idx]\n",
    "        # 修复：去掉 self. ，直接调用外部函数 selfies_to_sequence\n",
    "        sequence = selfies_to_sequence(selfies, self.vocab)\n",
    "        input_seq = sequence[:-1]\n",
    "        target_seq = sequence[1:]\n",
    "        props = torch.tensor(self.properties_list[idx], dtype=torch.float32)\n",
    "        props_norm = (props - self.props_mean) / self.props_std\n",
    "        return input_seq, target_seq, props_norm\n",
    "\n",
    "### 5. DataLoader构建（避免卡住）\n",
    "# 构建数据集\n",
    "train_dataset = SELFIESDataset(train_selfies, train_properties, vocab)\n",
    "val_dataset = SELFIESDataset(val_selfies, val_properties, vocab)\n",
    "\n",
    "# 加载指纹（若已保存）\n",
    "try:\n",
    "    train_fingerprints = torch.load(\"train_fingerprints_50k.pt\")\n",
    "    print(f\"\\n✅ 指纹加载完成：形状={train_fingerprints.shape}\")\n",
    "except:\n",
    "    print(\"\\n⚠️  未找到指纹文件，跳过指纹加载\")\n",
    "\n",
    "# 修复DataLoader（单进程）\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "print(f\"训练集批次数量：{len(train_loader)}\")\n",
    "print(f\"验证集批次数量：{len(val_loader)}\")\n",
    "\n",
    "### 6. 批次加载测试\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"测试加载第一个批次...\")\n",
    "print(\"=\"*50)\n",
    "for batch in train_loader:\n",
    "    input_seq, target_seq, props_norm = batch\n",
    "    print(f\"✅ 批次加载成功！\")\n",
    "    print(f\"  - 输入序列形状：{input_seq.shape}\")\n",
    "    print(f\"  - 目标序列形状：{target_seq.shape}\")\n",
    "    print(f\"  - 性质标签形状：{props_norm.shape}\")\n",
    "    break\n",
    "\n",
    "# 输出关键信息\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Baseline数据准备完成！关键信息：\")\n",
    "print(f\"- 词汇表大小：{vocab_size}\")\n",
    "print(f\"- 训练集批次：{len(train_loader)}（batch_size={BATCH_SIZE}）\")\n",
    "print(f\"- 分子性质维度：{len(train_properties[0])}\")\n",
    "print(f\"- 分子指纹维度：{FINGERPRINT_NBITS}\")\n",
    "print(\"=\"*50)\n"
   ],
   "id": "f8786de83c1ca2a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "加载预处理结果（避免重复计算）...\n",
      "==================================================\n",
      "本地无预处理缓存，执行第一次预处理...\n",
      "\n",
      "预处理训练集...\n",
      "使用 12 个线程处理 100000 条数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "预处理SMILES→SELFIES+性质计算:   0%|          | 486/100000 [00:08<23:28, 70.63it/s][16:16:57] UFFTYPER: Unrecognized atom type: Se2+2 (20)\n",
      "预处理SMILES→SELFIES+性质计算:   4%|▍         | 3789/100000 [01:12<30:38, 52.34it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mIndexError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\transformers_env\\Lib\\multiprocessing\\pool.py:856\u001B[39m, in \u001B[36mIMapIterator.next\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    855\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m856\u001B[39m     item = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_items\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpopleft\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    857\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m:\n",
      "\u001B[31mIndexError\u001B[39m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     21\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m预处理训练集...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m counter.value = \u001B[32m0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m train_processed = \u001B[43mbatch_process_smiles\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_smiles_raw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m预处理验证集...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     25\u001B[39m counter.value = \u001B[32m0\u001B[39m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 103\u001B[39m, in \u001B[36mbatch_process_smiles\u001B[39m\u001B[34m(smiles_list, n_workers)\u001B[39m\n\u001B[32m    101\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m使用 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_workers\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m 个线程处理 \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(smiles_list)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m 条数据...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m ThreadPool(processes=n_workers) \u001B[38;5;28;01mas\u001B[39;00m pool:\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m     processed_results = \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtqdm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    104\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpool\u001B[49m\u001B[43m.\u001B[49m\u001B[43mimap_unordered\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcalculate_chemical_properties_cached\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msmiles_list\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    105\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtotal\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msmiles_list\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    106\u001B[39m \u001B[43m        \u001B[49m\u001B[43mdesc\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m预处理SMILES→SELFIES+性质计算\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    107\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmininterval\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.5\u001B[39;49m\n\u001B[32m    108\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m processed_data = []\n\u001B[32m    110\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m processed_results:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\transformers_env\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\transformers_env\\Lib\\multiprocessing\\pool.py:861\u001B[39m, in \u001B[36mIMapIterator.next\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    859\u001B[39m     \u001B[38;5;28mself\u001B[39m._pool = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    860\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m861\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_cond\u001B[49m\u001B[43m.\u001B[49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    862\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    863\u001B[39m     item = \u001B[38;5;28mself\u001B[39m._items.popleft()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Anaconda\\envs\\transformers_env\\Lib\\threading.py:355\u001B[39m, in \u001B[36mCondition.wait\u001B[39m\u001B[34m(self, timeout)\u001B[39m\n\u001B[32m    353\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[32m    354\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m355\u001B[39m         \u001B[43mwaiter\u001B[49m\u001B[43m.\u001B[49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    356\u001B[39m         gotit = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    357\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4、VAE模型定义\n",
    "由编码器+解码器组成，负责“学习分子规律+生成新分子”"
   ],
   "id": "796e3ef1c29081cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:18:02.266386Z",
     "start_time": "2025-12-15T08:18:02.266386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 1. 编码器\n",
    "class VAEEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, latent_dim=64, max_len=MAX_SELFIES_LEN):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc_mu = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim * 2, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embed)\n",
    "        hidden = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "### 2. 解码器\n",
    "class VAEDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, latent_dim=64, max_len=MAX_SELFIES_LEN):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim + latent_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.SOS_idx = vocab[\"<SOS>\"]\n",
    "        self.EOS_idx = vocab[\"<EOS>\"]\n",
    "        self.PAD_idx = vocab[\"<PAD>\"]\n",
    "\n",
    "    def forward(self, z, target_seq=None):\n",
    "        if target_seq is not None:\n",
    "            batch_size = target_seq.shape[0]\n",
    "            embed = self.embedding(target_seq[:, :-1])\n",
    "            z_repeat = z.unsqueeze(1).repeat(1, embed.shape[1], 1)\n",
    "            lstm_in = torch.cat([embed, z_repeat], dim=-1)\n",
    "            lstm_out, _ = self.lstm(lstm_in)\n",
    "            logits = self.fc_out(lstm_out)\n",
    "            return logits\n",
    "\n",
    "        batch_size = z.shape[0]\n",
    "        generated_seq = torch.full((batch_size, 1), self.SOS_idx, dtype=torch.long, device=z.device)\n",
    "        hidden = None\n",
    "        for _ in range(self.max_len - 1):\n",
    "            embed = self.embedding(generated_seq)\n",
    "            z_repeat = z.unsqueeze(1).repeat(1, embed.shape[1], 1)\n",
    "            lstm_in = torch.cat([embed, z_repeat], dim=-1)\n",
    "            lstm_out, hidden = self.lstm(lstm_in, hidden)\n",
    "            logits = self.fc_out(lstm_out[:, -1:, :])\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            generated_seq = torch.cat([generated_seq, next_token], dim=1)\n",
    "            if (next_token == self.EOS_idx).all():\n",
    "                break\n",
    "        return generated_seq\n",
    "\n",
    "### 3. 完整VAE模型（含生成方法）\n",
    "# 兜底词汇表（防止未定义）\n",
    "if 'vocab' not in locals():\n",
    "    vocab = defaultdict(int, {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2})\n",
    "    print(\" 兜底构建基础词汇表，建议使用预处理生成的完整 vocab！\")\n",
    "\n",
    "class MolVAE(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, latent_dim=64, max_seq_len=99):\n",
    "        super(MolVAE, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
    "        # 编码器（双向LSTM）\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(2 * hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(2 * hidden_dim, latent_dim)\n",
    "        # 解码器（单向LSTM）\n",
    "        self.fc_latent = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            bidirectional=False,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.encoder_lstm(embed)\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decode(self, z):\n",
    "        batch_size = z.shape[0]\n",
    "        hidden = self.fc_latent(z)\n",
    "        hidden = torch.stack([hidden, hidden], dim=0)\n",
    "        cell = torch.zeros_like(hidden)\n",
    "        sos_token = torch.tensor([self.vocab[\"<SOS>\"]] * batch_size, device=z.device).unsqueeze(1)\n",
    "        current_embed = self.embedding(sos_token)\n",
    "        outputs = []\n",
    "        for _ in range(self.max_seq_len):\n",
    "            lstm_out, (hidden, cell) = self.decoder_lstm(current_embed, (hidden, cell))\n",
    "            logit = self.fc_out(lstm_out)\n",
    "            outputs.append(logit)\n",
    "            next_token = logit.argmax(dim=-1)\n",
    "            current_embed = self.embedding(next_token)\n",
    "        logits = torch.cat(outputs, dim=1)\n",
    "        return logits\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits = self.decode(z)\n",
    "        return logits, mu, logvar\n",
    "\n",
    "    def generate(self, num_samples, device=\"cuda\"):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(num_samples, self.latent_dim, device=device)\n",
    "            logits = self.decode(z)\n",
    "            generated_seqs = logits.argmax(dim=-1)\n",
    "        return generated_seqs\n"
   ],
   "id": "db6a49b1f1f9ccd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5、模型训练与加载",
   "id": "214db510a3ace67"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:18:02.267363Z",
     "start_time": "2025-12-15T08:18:02.266386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 检查必要组件\n",
    "if 'train_loader' not in locals() or 'val_loader' not in locals():\n",
    "    raise ValueError(\"请先运行「预处理+DataLoader构建」的代码，确保 train_loader/val_loader 已定义！\")\n",
    "\n",
    "# 加载或训练模型\n",
    "if os.path.exists(model_path):\n",
    "    print(f\" 加载已保存的最优模型：{model_path}\")\n",
    "    vae_model = MolVAE(vocab_size=len(vocab), max_seq_len=99)\n",
    "    vae_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    vae_model.to(device)\n",
    "    trained_model = vae_model\n",
    "    print(\"模型加载完成，可直接生成分子！\")\n",
    "else:\n",
    "    print(f\" 未找到保存的模型，开始训练...\")\n",
    "    def train_vae(model, train_loader, val_loader, epochs=2, lr=1e-3, device=\"cuda\"):\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])\n",
    "        # 优化器调用适配修复后的导入（无需修改）\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "        best_val_loss = float(\"inf\")\n",
    "        early_stop_patience = 5\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # 训练阶段\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "                input_seq, target_seq, _ = batch\n",
    "                input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits, mu, logvar = model(input_seq)\n",
    "                assert logits.shape[1] == target_seq.shape[1], f\"长度不匹配：{logits.shape[1]} vs {target_seq.shape[1]}\"\n",
    "                recon_loss = criterion(logits.reshape(-1, logits.shape[-1]), target_seq.reshape(-1))\n",
    "                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / input_seq.shape[0]\n",
    "                total_loss = recon_loss + 0.01 * kl_loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                train_loss += total_loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "\n",
    "            # 验证阶段\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} (Val)\"):\n",
    "                    input_seq, target_seq, _ = batch\n",
    "                    input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "                    logits, mu, logvar = model(input_seq)\n",
    "                    recon_loss = criterion(logits.reshape(-1, logits.shape[-1]), target_seq.reshape(-1))\n",
    "                    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / input_seq.shape[0]\n",
    "                    total_loss = recon_loss + 0.01 * kl_loss\n",
    "                    val_loss += total_loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            # 日志输出\n",
    "            print(f\"\\nEpoch {epoch+1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "            # 早停和模型保存\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                early_stop_counter = 0\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"  → 保存最优模型（Val Loss: {best_val_loss:.4f}）\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "                print(f\"  → 早停计数器：{early_stop_counter}/{early_stop_patience}\")\n",
    "                if early_stop_counter >= early_stop_patience:\n",
    "                    print(\"  → 早停触发，训练结束！\")\n",
    "                    break\n",
    "\n",
    "        # 加载最优模型\n",
    "        model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "        return model\n",
    "\n",
    "    # 启动训练\n",
    "    vae_model = MolVAE(vocab_size=len(vocab), max_seq_len=99)\n",
    "    trained_model = train_vae(vae_model, train_loader, val_loader, device=device)\n",
    "    print(f\" 训练完成！最优模型已保存为 {model_path}\")\n"
   ],
   "id": "b853042d61a32440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6、分子生成函数（去重以及生成变量供后续评估使用）",
   "id": "f1c9c86abcffac11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-15T08:18:02.267363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_molecules(model, num_samples=100, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    generated_smiles_before_dedup = []  # 去重前的结果（用于唯一性计算）\n",
    "    generated_smiles_after_dedup = []\n",
    "    idx_to_char = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        batch_size = 64\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            curr_batch_size = min(batch_size, num_samples - i)\n",
    "            generated_seqs = model.generate(curr_batch_size, device)\n",
    "\n",
    "            for seq in generated_seqs.cpu().numpy():\n",
    "                valid_idx = (seq != vocab[\"<SOS>\"]) & (seq != vocab[\"<EOS>\"]) & (seq != vocab[\"<PAD>\"])\n",
    "                seq = seq[valid_idx]\n",
    "                if len(seq) == 0 or len(seq) > 100:\n",
    "                    continue\n",
    "                try:\n",
    "                    selfies_chars = [idx_to_char[idx] for idx in seq]\n",
    "                    selfies = \"\".join(selfies_chars)\n",
    "                    smiles = sf.decoder(selfies)\n",
    "                    if smiles and filter_unreasonable_smiles(smiles):\n",
    "                        generated_smiles_before_dedup.append(smiles)  # 去重前保存\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    # 去重\n",
    "    generated_smiles_after_dedup = list(set(generated_smiles_before_dedup))\n",
    "    print(f\"生成分子完成：\")\n",
    "    print(f\"  - 目标生成数量：{num_samples}\")\n",
    "    print(f\"  - 去重前有效分子：{len(generated_smiles_before_dedup)} 条\")\n",
    "    print(f\"  - 去重后有效分子：{len(generated_smiles_after_dedup)} 条\")\n",
    "    print(f\"  - 初始有效率：{len(generated_smiles_before_dedup)/num_samples*100:.1f}%\")\n",
    "\n",
    "    # 保存结果\n",
    "    temp_df = pd.DataFrame({\"Generated_SMILES\": generated_smiles_after_dedup})\n",
    "    temp_df.to_csv(\"temp_generated_smiles.csv\", index=False)\n",
    "    print(f\"  - 生成结果已保存至：temp_generated_smiles.csv\")\n",
    "\n",
    "    # 返回去重前后结果（关键：给后续评估提供变量）\n",
    "    return generated_smiles_before_dedup, generated_smiles_after_dedup\n",
    "\n",
    "# 执行生成（必须先运行这一步，才能生成评估所需的变量）\n",
    "generated_smiles_before_dedup, generated_smiles_after_dedup = generate_molecules(trained_model, num_samples=1000, device=device)\n"
   ],
   "id": "d7d0a0b4e60cc1ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7、分子评估函数（合理性+新颖性+多样性+唯一性+有效性）",
   "id": "ff3180f54677bbeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T08:18:02.268340Z",
     "start_time": "2025-12-15T08:18:02.268340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### 1. 指纹计算函数\n",
    "def calculate_fingerprint(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    sparse_fp = AllChem.GetHashedMorganFingerprint(\n",
    "        mol, radius=FINGERPRINT_RADIUS, nBits=FINGERPRINT_NBITS\n",
    "    )\n",
    "    dense_fp = np.zeros(FINGERPRINT_NBITS, dtype=np.int32)\n",
    "    for idx, count in sparse_fp.GetNonzeroElements().items():\n",
    "        dense_fp[idx] = count\n",
    "    return dense_fp\n",
    "\n",
    "### 2. 化学合理性评估（有效性核心）\n",
    "def evaluate_chemical_validity(smiles_list):\n",
    "    valid_count = 0\n",
    "    valid_smiles = []\n",
    "    for smiles in tqdm(smiles_list, desc=\"评估化学合理性\"):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            continue\n",
    "        # 验证无超价原子\n",
    "        has_supervalent = False\n",
    "        atom_constraints = {\n",
    "            \"S\": 4, \"P\": 5, \"Cl\": 3, \"Br\": 5, \"I\": 6,\n",
    "            \"Se\": 4, \"Te\": 6, \"As\": 5, \"Si\": 6, \"B\": 4\n",
    "        }\n",
    "        for atom in mol.GetAtoms():\n",
    "            symbol = atom.GetSymbol()\n",
    "            valence = atom.GetExplicitValence()\n",
    "            if symbol in atom_constraints and valence > atom_constraints[symbol]:\n",
    "                has_supervalent = True\n",
    "                break\n",
    "        if not has_supervalent:\n",
    "            valid_count += 1\n",
    "            valid_smiles.append(smiles)\n",
    "    validity = valid_count / len(smiles_list) * 100 if len(smiles_list) > 0 else 0.0\n",
    "    print(f\"\\n1. 化学合理性：{validity:.1f}%（{valid_count}/{len(smiles_list)} 个有效）\")\n",
    "    return validity, valid_smiles\n",
    "\n",
    "### 3. 新颖性评估\n",
    "def evaluate_novelty(generated_smiles, train_smiles, val_smiles, threshold=0.8):\n",
    "    all_known_smiles = train_smiles + val_smiles\n",
    "    known_fps = []\n",
    "    for smiles in tqdm(all_known_smiles[:10000], desc=\"计算已知分子指纹（抽样1万条加速）\"):\n",
    "        fp = calculate_fingerprint(smiles)\n",
    "        if fp is not None:\n",
    "            known_fps.append(fp)\n",
    "    known_fps = np.array(known_fps)\n",
    "    if len(known_fps) == 0:\n",
    "        print(\"警告：已知分子无有效指纹，新颖性默认为100%\")\n",
    "        return 100.0, generated_smiles\n",
    "\n",
    "    novel_count = 0\n",
    "    novel_smiles = []\n",
    "    for smiles in tqdm(generated_smiles, desc=\"评估新颖性\"):\n",
    "        fp = calculate_fingerprint(smiles)\n",
    "        if fp is None:\n",
    "            continue\n",
    "        max_similarity = 0.0\n",
    "        for known_fp in known_fps:\n",
    "            intersection = np.sum(np.logical_and(fp, known_fp))\n",
    "            union = np.sum(np.logical_or(fp, known_fp))\n",
    "            similarity = intersection / union if union != 0 else 0.0\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                if max_similarity >= threshold:\n",
    "                    break\n",
    "        if max_similarity < threshold:\n",
    "            novel_count += 1\n",
    "            novel_smiles.append(smiles)\n",
    "\n",
    "    novelty = novel_count / len(generated_smiles) * 100 if len(generated_smiles) > 0 else 0.0\n",
    "    print(f\"2. 新颖性：{novelty:.1f}%（{novel_count}/{len(generated_smiles)} 个新颖）\")\n",
    "    return novelty, novel_smiles\n",
    "\n",
    "### 4. 多样性评估\n",
    "def evaluate_diversity(generated_smiles, threshold=0.7):\n",
    "    generated_fps = []\n",
    "    for smiles in tqdm(generated_smiles, desc=\"计算生成分子指纹\"):\n",
    "        fp = calculate_fingerprint(smiles)\n",
    "        if fp is not None:\n",
    "            generated_fps.append(fp)\n",
    "    generated_fps = np.array(generated_fps)\n",
    "\n",
    "    if len(generated_fps) < 2:\n",
    "        print(\"3. 多样性：0.0%（生成分子数量不足或无有效指纹）\")\n",
    "        return 0.0\n",
    "\n",
    "    similarities = []\n",
    "    n = len(generated_fps)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            intersection = np.sum(np.logical_and(generated_fps[i], generated_fps[j]))\n",
    "            union = np.sum(np.logical_or(generated_fps[i], generated_fps[j]))\n",
    "            sim = intersection / union if union != 0 else 0.0\n",
    "            similarities.append(sim)\n",
    "\n",
    "    avg_similarity = np.mean(similarities)\n",
    "    diversity = (1 - avg_similarity) * 100\n",
    "    print(f\"3. 多样性：{diversity:.1f}%（平均相似度：{avg_similarity:.3f}）\")\n",
    "    return diversity\n",
    "### 唯一性评判（新增）\n",
    "def evaluate_uniqueness(generated_smiles_before_dedup, generated_smiles_after_dedup):\n",
    "    total_valid_before_dedup = len(generated_smiles_before_dedup)\n",
    "    total_unique_after_dedup = len(generated_smiles_after_dedup)\n",
    "\n",
    "    if total_valid_before_dedup == 0:\n",
    "        uniqueness = 0.0\n",
    "    else:\n",
    "        uniqueness = (total_unique_after_dedup / total_valid_before_dedup) * 100\n",
    "\n",
    "    print(f\"4. 唯一性：{uniqueness:.1f}%\")\n",
    "    print(f\"  - 去重前有效分子数：{total_valid_before_dedup}\")\n",
    "    print(f\"  - 去重后唯一分子数：{total_unique_after_dedup}\")\n",
    "    print(f\"  - 重复分子数：{total_valid_before_dedup - total_unique_after_dedup}\")\n",
    "    return uniqueness\n",
    "\n",
    "### 有效性补充评估（Lipinski规则符合性，新增：修复非法字符）\n",
    "def evaluate_lipinski_compliance(smiles_list):\n",
    "    \"\"\"评估分子是否符合Lipinski四规则（类药性质有效性）\"\"\"\n",
    "    compliant_count = 0\n",
    "    compliant_smiles = []\n",
    "    for smiles in tqdm(smiles_list, desc=\"评估Lipinski规则符合性\"):\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if not mol:\n",
    "            continue\n",
    "        # Lipinski四规则\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        logp = Descriptors.MolLogP(mol)\n",
    "        h_donors = Lipinski.NumHDonors(mol)\n",
    "        h_acceptors = Lipinski.NumHAcceptors(mol)\n",
    "\n",
    "        # 满足≥3条规则即视为符合（修复：≤→<=，≥→>=）\n",
    "        rules_met = 0\n",
    "        if mw < 500: rules_met += 1\n",
    "        if logp < 5: rules_met += 1\n",
    "        if h_donors <= 5: rules_met += 1  # 已修复：≤→<=\n",
    "        if h_acceptors <= 10: rules_met += 1  # 已修复：≤→<=\n",
    "\n",
    "        if rules_met >= 3:  # 已修复：≥→>=\n",
    "            compliant_count += 1\n",
    "            compliant_smiles.append(smiles)\n",
    "\n",
    "    compliance_rate = compliant_count / len(smiles_list) * 100 if len(smiles_list) > 0 else 0.0\n",
    "    print(f\"5. Lipinski规则符合性（类药有效性）：{compliance_rate:.1f}%（{compliant_count}/{len(smiles_list)} 个符合）\")\n",
    "    return compliance_rate, compliant_smiles\n"
   ],
   "id": "476bb8516a1cfbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 8、执行完整评估",
   "id": "f23a09dcb7ce64aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-15T08:18:02.268340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载生成分子（双重保险：从文件加载，避免变量丢失）\n",
    "temp_df = pd.read_csv(\"temp_generated_smiles.csv\")\n",
    "generated_smiles = temp_df[\"Generated_SMILES\"].tolist()\n",
    "print(f\"加载生成分子：共 {len(generated_smiles)} 条（去重后）\")\n",
    "\n",
    "# 确保训练/验证集有效分子已加载\n",
    "try:\n",
    "    print(f\"训练集有效分子：{len(train_smiles_valid)} 条 | 验证集有效分子：{len(val_smiles_valid)} 条\")\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"dataset3/dataset3.csv\")\n",
    "    df.rename(columns={\"0\": \"smiles\", \"1\": \"label\"}, inplace=True)\n",
    "    np.random.seed(42)\n",
    "    original_smiles = df[\"smiles\"].tolist()\n",
    "    np.random.shuffle(original_smiles)\n",
    "\n",
    "    train_smiles_raw = original_smiles[:TRAIN_SIZE]\n",
    "    val_smiles_raw = original_smiles[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE]\n",
    "    train_smiles_valid = [s for s in train_smiles_raw if filter_unreasonable_smiles(s)]\n",
    "    val_smiles_valid = [s for s in val_smiles_raw if filter_unreasonable_smiles(s)]\n",
    "    print(f\"重新加载有效分子：训练集 {len(train_smiles_valid)} 条 | 验证集 {len(val_smiles_valid)} 条\")\n",
    "\n",
    "# 执行评估（此时 generated_smiles_before_dedup 已在第七部分定义）\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"开始五维度评估...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 化学合理性（基础有效性）\n",
    "validity, valid_generated = evaluate_chemical_validity(generated_smiles)\n",
    "\n",
    "# 2. 新颖性\n",
    "novelty, novel_generated = evaluate_novelty(valid_generated, train_smiles_valid, val_smiles_valid)\n",
    "\n",
    "# 3. 多样性\n",
    "diversity = evaluate_diversity(novel_generated)\n",
    "\n",
    "# 4. 唯一性（新增：变量已定义）\n",
    "uniqueness = evaluate_uniqueness(generated_smiles_before_dedup, generated_smiles_after_dedup)\n",
    "\n",
    "# 5. Lipinski规则符合性（类药有效性，新增）\n",
    "lipinski_compliance, lipinski_compliant_smiles = evaluate_lipinski_compliance(novel_generated)\n",
    "\n",
    "# 输出最终报告\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"生成分子五维度评估报告\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. 化学合理性（基础有效性）：{validity:.1f}%（合格线≥60%，理想线≥75%）\")\n",
    "print(f\"2. 新颖性：{novelty:.1f}%（合格线≥50%，理想线≥70%）\")\n",
    "print(f\"3. 多样性：{diversity:.1f}%（合格线≥40%，理想线≥60%）\")\n",
    "print(f\"4. 唯一性：{uniqueness:.1f}%（合格线≥80%，理想线≥95%）\")\n",
    "print(f\"5. Lipinski规则符合性（类药有效性）：{lipinski_compliance:.1f}%（合格线≥60%，理想线≥80%）\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 保存最终有效+新颖+类药分子\n",
    "final_df = pd.DataFrame({\n",
    "    \"Generated_SMILES\": lipinski_compliant_smiles,\n",
    "    \"Evaluation_Result\": \"Valid_Novel_Lipinski_Compliant\"\n",
    "})\n",
    "final_df.to_csv(\"final_generated_molecules.csv\", index=False)\n",
    "print(f\"\\n评估完成！最终有效分子保存为 final_generated_molecules.csv（共 {len(lipinski_compliant_smiles)} 条）\")\n"
   ],
   "id": "ddbe20d7deb94b16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 9、分子绘制（PyCharm Jupyter Notebook分子结构图展示）",
   "id": "f7531ead946eeecf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-15T08:18:02.269345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_molecule_structures(smiles_list, num_display=5):\n",
    "    \"\"\"\n",
    "    在PyCharm Jupyter Notebook中展示分子结构图\n",
    "    :param smiles_list: 有效SMILES列表\n",
    "    :param num_display: 展示的分子数量\n",
    "    \"\"\"\n",
    "    # 筛选有效SMILES（防止绘制失败）\n",
    "    valid_smiles_for_display = []\n",
    "    for smiles in smiles_list[:num_display]:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            valid_smiles_for_display.append(smiles)\n",
    "\n",
    "    if not valid_smiles_for_display:\n",
    "        print(\"无有效分子可展示\")\n",
    "        return\n",
    "\n",
    "    # 绘制分子结构（支持批量展示）\n",
    "    mols = [Chem.MolFromSmiles(smiles) for smiles in valid_smiles_for_display]\n",
    "    img = Draw.MolsToGridImage(\n",
    "        mols,\n",
    "        molsPerRow=2,  # 每行展示2个\n",
    "        subImgSize=(300, 300),  # 每个分子图大小\n",
    "        legends=valid_smiles_for_display  # 显示SMILES作为图例\n",
    "    )\n",
    "\n",
    "    # 在Jupyter中显示图片\n",
    "    display(img)\n",
    "    print(f\"\\n展示 {len(valid_smiles_for_display)} 个生成分子的结构图（共筛选前 {num_display} 个有效分子）\")\n",
    "\n",
    "# 执行展示（需在PyCharm Jupyter Notebook中运行）\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PyCharm Jupyter Notebook分子结构图展示\")\n",
    "print(\"=\"*60)\n",
    "display_molecule_structures(lipinski_compliant_smiles, num_display=5)"
   ],
   "id": "e7a7fc3dcd09eb9d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "transformers_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
